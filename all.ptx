//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Jun 19 20:27:30 2014 (1403224050)
// Cuda compilation tools, release 6.5, V6.5.10
//

//ptxas -v -arch=sm_52 -m 32 --opt-level 4 -o all.cubin all.ptx
//cuobjdump -sass all.cubin > all.txt

.version 4.1
.target sm_52
.address_size 32


.visible .entry float(
    .param .u64 .ptr.global.align 16 param_p16,
    .param .u64 .ptr.global.align 8 param_p8,
    .param .u64 .ptr.global.align 4 param_p4,
    .param .u64 .ptr.global.align 2 param_p2,
    .param .u64 .ptr.global.align 1 param_p1,
    .param .u64 .ptr.const.align 16 param_c16,
    .param .u64 .ptr.const.align 8 param_c8,
    .param .u64 .ptr.const.align 4 param_c4,
    .param .u64 .ptr.const.align 2 param_c2,
    .param .u64 .ptr.const.align 1 param_c1,
    .param .u32 .ptr.global.align 4 param_out,
    .param .u32 .ptr.global.align 8 param_out8,
    .param .u32 .ptr.global.align 16 param_out16,
    .param .u32 .ptr.global.align 32 param_out32,
    .param .f32 param_in0,
    .param .f32 param_in1,
    .param .u8 param_u8,
    .param .s8 param_s8,
    .param .u16 param_u16,
    .param .s16 param_s16,
    .param .u64 param_u64,
    .param .s64 param_s64
)
{
    .reg .u32 out;
    .reg .f32 in<2>, r<3>;
    .reg .f16 f16;
    .reg .s32 s32;
    .reg .u32 u32;
    .reg .s16 s16;
    .reg .u16 u16;
    .reg .s8 s8;
    .reg .u8 u8;
    .reg .pred p<7>;

    ld.param.u32 out, [param_out];
    ld.param.f32 in0, [param_in0];
    ld.param.f32 in1, [param_in1];

    // add{.rnd}{.ftz}{.sat}.f32 d, a, b;
    // .rnd = { .rn, .rz, .rm, .rp };

    // const
    add.rn.f32 r0, in0, in1;
    neg.f32 r1, in1;
    add.rn.f32 r0, r0, r1;
    abs.f32 r1, in1;
    add.rn.f32 r0, r0, r1;

    // 32I
    add.rn.f32 r0, r0, 0F00000001;
    add.rn.f32 r0, r0, 0F80000001;
    add.rn.ftz.f32 r0, r0, 0F00000002;

    // Rounding modes
    add.rn.f32 r0, r0, 0.0;
    add.rz.f32 r0, 0.0, r0;
    add.rm.f32 r0, r0, r0;
    add.rp.f32 r0, r0, r0;

    // ftz/sat
    add.rn.ftz.f32 r0, r0, r0;
    add.rz.ftz.sat.f32 r0, r0, r0;

    // pos/neg
    add.rn.f32 r0, r0, 1.5;
    add.rn.f32 r0, r0, -1.5;

    // abs op
    abs.f32 r1, r0;
    add.rn.f32 r0, r1, r0;
    abs.f32 r1, r0;
    add.rn.f32 r0, r0, r1;

    // neg op
    neg.f32 r1, r0;
    add.rn.f32 r0, r1, r0;
    neg.f32 r1, r0;
    add.rn.f32 r0, r0, r1;

    //mul{.rnd}{.ftz}{.sat}.f32 d, a, b;
    //.rnd = { .rn, .rz, .rm, .rp };

    // const
    mul.rn.f32 r0, r0, in1;
    neg.f32 r1, in1;
    mul.rn.f32 r0, r0, r1;

    // 32I
    mul.rn.f32 r0, r0, 0F80000001;
    mul.rn.ftz.f32 r0, r0, 0F80000002;

    // Rounding modes
    mul.rn.f32 r0, r0, 0.0;
    mul.rz.f32 r0, 0.0, r0;
    mul.rm.f32 r0, r0, r0;
    mul.rp.f32 r0, r0, r0;

    // ftz/sat
    mul.rn.ftz.f32 r0, r0, r0;
    mul.rz.ftz.sat.f32 r0, r0, r0;

    // pos/neg
    mul.rn.f32 r0, r0, 1.5;
    mul.rn.f32 r0, r0, -1.5;

    // neg op
    neg.f32 r1, r0;
    mul.rn.f32 r0, r1, r0;
    neg.f32 r1, r0;
    mul.rn.f32 r0, r0, r1;


    //fma.rnd{.ftz}{.sat}.f32 d, a, b, c;
    //.rnd = { .rn, .rz, .rm, .rp };

    fma.rn.f32 r0, r0, in1, r0;
    fma.rn.f32 r0, r0, r0, in1;

    fma.rn.f32 r0, r0, 1.5, r0;
    fma.rn.f32 r0, r0, -1.5, r0;

    fma.rn.f32 r0, r0, r0, 1.5;
    fma.rn.f32 r0, r0, r0, -1.5;

    fma.rn.ftz.f32 r0, r0, r0, r0;
    fma.rz.ftz.sat.f32 r0, r0, r0, r0;
    fma.rn.f32 r0, r0, r0, 0.0;
    fma.rz.f32 r0, r0, 0.0, r0;
    fma.rm.f32 r0, 0.0, r0, r0;
    fma.rp.f32 r0, r0, r0, r0;

    neg.f32 r1, r0;
    fma.rn.f32 r0, r0, r1, r0;
    neg.f32 r1, r0;
    fma.rn.f32 r0, r0, r0, r1;

    //min{.ftz}.f32 d, a, b;
    //max{.ftz}.f32 d, a, b;

    min.f32 r0, r0, in1;

    min.f32 r0, r0, r0;
    max.ftz.f32 r0, r0, r0;

    min.f32 r0, r0, 1.5;
    max.f32 r0, r0, -1.5;

    neg.f32 r1, r0;
    min.f32 r0, r0, r1;
    neg.f32 r1, r0;
    max.f32 r0, r1, r0;

    abs.f32 r1, r0;
    max.f32 r0, r0, r1;
    abs.f32 r1, r0;
    min.f32 r0, r1, r0;

    //rcp.approx.ftz.f32 d, a; // fast, approximate reciprocal

    rcp.approx.ftz.f32 r0, r0;
    neg.f32 r1, r0;
    rcp.approx.ftz.f32 r0, r1;

    //rsqrt.approx.ftz.f32 d, a; // fast, approximate reciprocal square root
    rsqrt.approx.ftz.f32 r0, r0;
    neg.f32 r1, r0;
    rsqrt.approx.ftz.f32 r0, r1;

    //sin.approx{.ftz}.f32  d, a;
    sin.approx.f32 r0, r0;
    neg.f32 r1, r0;
    sin.approx.f32 r0, r1;

    //cos.approx{.ftz}.f32  d, a;
    cos.approx.f32 r0, r0;
    neg.f32 r1, r0;
    cos.approx.f32 r0, r1;

    //lg2.approx{.ftz}.f32  d, a;
    lg2.approx.ftz.f32 r0, r0;
    neg.f32 r1, r0;
    lg2.approx.ftz.f32 r0, r1;

    //ex2.approx{.ftz}.f32  d, a;
    ex2.approx.ftz.f32 r0, r0;
    neg.f32 r1, r0;
    ex2.approx.ftz.f32 r0, r1;
    add.rn.f32 r1, r0, r0;
    add.rn.f32 r2, r1, r1;
    ex2.approx.ftz.f32 r0, r2;
    add.rn.f32 r0, r0, r1;

    //setp.CmpOp{.ftz}.type p[|q], a, b;
    //setp.CmpOp.BoolOp{.ftz}.type p[|q], a, b, {!}c;
    //.CmpOp = { eq, ne, lt, le, gt, ge, equ, neu, ltu, leu, gtu, geu, num, nan };
    //.BoolOp = { and, or, xor };
    //.type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 };

    add.rn.f32 r1, r0, r0;
    setp.lt.ftz.f32 p0, r1, r0;

    add.rn.f32 r0, r0, r0;
    setp.lt.and.f32 p0, r1, r0, !p0;
    add.rn.f32 r0, r0, r0;
    setp.lt.or.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.lt.xor.f32 p0, r1, r0, p0;

    add.rn.f32 r0, r0, r0;
    setp.le.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.eq.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.ne.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.gt.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.ge.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.num.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.nan.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.ltu.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.leu.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.equ.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.neu.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.gtu.and.f32 p0, r1, r0, p0;
    add.rn.f32 r0, r0, r0;
    setp.geu.and.f32 p0, r1, r0, p0;

    add.rn.f32 r0, r0, r0;
    setp.lt.and.f32 p0, r0, in1, p0;

    neg.f32 r1, in1;
    setp.lt.and.f32 p0, r0, r1, p0;

    neg.f32 r1, r0;
    setp.lt.and.f32 p0, r0, r1, p0;
    add.rn.f32 r0, r0, r0;
    setp.lt.and.f32 p0, r1, r0, p0;

    abs.f32 r1, r0;
    setp.lt.and.f32 p0, r1, r0, p0;
    abs.f32 r1, r0;
    setp.lt.and.f32 p0, r0, r1, p0;

    add.rn.f32 r0, r0, r0;
    setp.lt.and.f32 p0, r0, 1.5, p0;
    add.rn.f32 r0, r0, r0;
    setp.lt.and.f32 p0, r0, -1.5, p0;

    @p0 add.rn.f32 r0, r0, r0;

    set.lt.and.ftz.f32.f32  r0, r0, r1, p0;
    set.lt.and.f32.f32  r0, r0, r1, p0;
    set.gt.and.f32.f32  r0, r0, in1, p0;
    set.lt.or.f32.f32  r0, r0, 1.5, p0;
    set.lt.xor.f32.f32  r0, r0, -1.5, p0;

    //selp.type d, a, b, c;
    //selp.f32 r0, r0, r1, p0;

    // This behaves just like FSETP combined with FFMA R3 op width
    //neg.f32 r1, r0;
    add.rn.f32 r1, r0, r0;
    slct.ftz.f32.f32  r0, r0, 1.5, r0;


    //cvt{.frnd}{.ftz}{.sat}.dtype.atype d, a; // fp rounding
    //.frnd = { .rn, .rz, .rm, .rp };
    //.irnd = { .rni, .rzi, .rmi, .rpi };
    //.dtype = .atype = { .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f16, .f32, .f64 };

    cvt.f32.f32 r0, r0;
    cvt.rni.ftz.sat.f32.f32 r0, r0;
    cvt.ftz.f32.f32 r0, r0;
    abs.f32 r0, r0;
    cvt.f32.f32 r0, r0;
    neg.f32 r0, r0;
    cvt.f32.f32 r0, r0;

    cvt.rni.f32.f32 r0, r0;
    cvt.rzi.f32.f32 r0, r0;
    cvt.rmi.f32.f32 r0, r0;
    cvt.rpi.f32.f32 r0, r0;

    cvt.rn.f16.f32 f16, r0;
    cvt.f32.f16 r0, f16;
    cvt.rz.f16.f32 f16, r0;
    cvt.f32.f16 r0, f16;
    cvt.rm.f16.f32 f16, r0;
    cvt.f32.f16 r0, f16;
    cvt.rp.f16.f32 f16, r0;
    cvt.f32.f16 r0, f16;

    cvt.rni.s32.f32 s32, r0;
    cvt.rn.f32.s32 r0, s32;
    cvt.rzi.s32.f32 s32, r0;
    cvt.rz.f32.s32 r0, s32;
    cvt.rmi.s32.f32 s32, r0;
    cvt.rm.f32.s32 r0, s32;
    cvt.rpi.s32.f32 s32, r0;
    cvt.rp.f32.s32 r0, s32;

    cvt.rni.u32.f32 u32, r0;
    cvt.rn.f32.u32 r0, u32;
    cvt.rzi.u32.f32 u32, r0;
    cvt.rz.f32.u32 r0, u32;
    cvt.rmi.u32.f32 u32, r0;
    cvt.rm.f32.u32 r0, u32;
    cvt.rpi.u32.f32 u32, r0;
    cvt.rp.f32.u32 r0, u32;

    cvt.rni.s16.f32 s16, r0;
    cvt.rn.f32.s16 r0, s16;
    cvt.rni.u16.f32 u16, r0;
    cvt.rn.f32.u16 r0, u16;
    cvt.rni.s8.f32 s8, r0;
    cvt.rn.f32.s8 r0, s8;
    cvt.rni.u8.f32 u8, r0;
    cvt.rn.f32.u8 r0, u8;

    st.global.wb.f32 [out+4], r0;
    st.global.wb.f32 [out+-4], r0;
    st.global.cg.f32 [out+4], r0;
    st.global.cs.f32 [out+8], r0;
    st.global.wt.f32 [out+12], r0;

    ret;
}

.visible .entry double(
    .param .u32 .ptr.global.align 8 param_out,
    .param .f64 param_in0
)
{
    .reg .u32 out;
    .reg .f64 in<2>, r<2>;
    .reg .f32 f32;
    .reg .s64 s64;
    .reg .u64 u64;
    .reg .s32 s32;
    .reg .u32 u32;
    .reg .pred p<7>;

    ld.param.u32 out, [param_out];
    ld.param.f64 in0, [param_in0];

    // add{.rnd}.f64 d, a, b;
    // .rnd = { .rn, .rz, .rm, .rp };

    // Rounding modes
    add.rn.f64 r0, in0, in0;
    add.rz.f64 r0, r0, r0;
    add.rm.f64 r0, r0, r0;
    add.rp.f64 r0, r0, r0;

    // pos/neg
    add.rn.f64 r0, r0, 1.5;
    add.rn.f64 r0, r0, -1.5;

    // abs op
    abs.f64 r1, r0;
    add.rn.f64 r0, r1, r0;
    abs.f64 r1, r0;
    add.rn.f64 r0, r0, r1;

    // neg op
    neg.f64 r1, r0;
    add.rn.f64 r0, r1, r0;
    neg.f64 r1, r0;
    add.rn.f64 r0, r0, r1;

    //mul{.rnd}.f64 d, a, b;
    //.rnd = { .rn, .rz, .rm, .rp };

    // Rounding modes
    mul.rn.f64 r0, r0, 0.0;
    mul.rz.f64 r0, 0.0, r0;
    mul.rm.f64 r0, r0, r0;
    mul.rp.f64 r0, r0, r0;

    // pos/neg
    mul.rn.f64 r0, r0, 1.5;
    mul.rn.f64 r0, r0, -1.5;

    // neg op
    neg.f64 r1, r0;
    mul.rn.f64 r0, r1, r0;
    neg.f64 r1, r0;
    mul.rn.f64 r0, r0, r1;

    //fma.rnd.f64 d, a, b, c;
    //.rnd = { .rn, .rz, .rm, .rp };

    fma.rn.f64 r0, r0, in1, r0;
    fma.rn.f64 r0, r0, r0, in1;
    fma.rn.f64 r0, r0, 1.5, r0;
    fma.rn.f64 r0, r0, -1.5, r0;
    fma.rn.f64 r0, r0, r0, r0;
    fma.rz.f64 r0, r0, r0, r0;
    fma.rm.f64 r0, r0, r0, r0;
    fma.rp.f64 r0, r0, r0, r0;

    neg.f64 r1, r0;
    fma.rn.f64 r0, r0, r1, r0;
    neg.f64 r1, r0;
    fma.rn.f64 r0, r0, r0, r1;

    //min.f64 d, a, b;
    //max.f64 d, a, b;

    min.f64 r0, r0, r0;

    min.f64 r0, r0, 1.5;
    max.f64 r0, r0, -1.5;

    neg.f64 r1, r0;
    min.f64 r0, r0, r1;
    neg.f64 r1, r0;
    max.f64 r0, r1, r0;

    abs.f64 r1, r0;
    max.f64 r0, r0, r1;
    abs.f64 r1, r0;
    min.f64 r0, r1, r0;

    //rcp.approx.ftz.f64 d, a; // fast, approximate reciprocal
    rcp.approx.ftz.f64 r0, r0;
    neg.f64 r1, r0;
    rcp.approx.ftz.f64 r0, r1;

    //rsqrt.approx.ftz.f64 d, a; // fast, approximate reciprocal square root
    rsqrt.approx.ftz.f64 r0, r0;
    neg.f64 r1, r0;
    rsqrt.approx.ftz.f64 r0, r1;

    //setp.CmpOp{.ftz}.type p[|q], a, b;
    //setp.CmpOp.BoolOp{.ftz}.type p[|q], a, b, {!}c;
    //.CmpOp = { eq, ne, lt, le, gt, ge, equ, neu, ltu, leu, gtu, geu, num, nan };
    //.BoolOp = { and, or, xor };
    //.type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64, .f32, .f64 };

    add.rn.f64 r1, r0, r0;
    setp.lt.f64 p0, r1, r0;

    add.rn.f64 r0, r0, r0;
    setp.lt.and.f64 p0, r1, r0, !p0;
    add.rn.f64 r0, r0, r0;
    setp.lt.or.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.lt.xor.f64 p0, r1, r0, p0;

    add.rn.f64 r0, r0, r0;
    setp.le.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.eq.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.ne.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.gt.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.ge.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.num.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.nan.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.ltu.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.leu.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.equ.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.neu.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.gtu.and.f64 p0, r1, r0, p0;
    add.rn.f64 r0, r0, r0;
    setp.geu.and.f64 p0, r1, r0, p0;

    neg.f64 r1, r0;
    setp.lt.and.f64 p0, r0, r1, p0;
    add.rn.f64 r0, r0, r0;
    setp.lt.and.f64 p0, r1, r0, p0;

    abs.f64 r1, r0;
    setp.lt.and.f64 p0, r1, r0, p0;
    abs.f64 r1, r0;
    setp.lt.and.f64 p0, r0, r1, p0;

    add.rn.f64 r0, r0, r0;
    setp.lt.and.f64 p0, r0, 1.5, p0;
    add.rn.f64 r0, r0, r0;
    setp.lt.and.f64 p0, r0, -1.5, p0;

    @p0 add.rn.f64 r0, r0, r0;

    set.lt.and.f32.f64  f32, r0, r1, p0;
    cvt.f64.f32 r0, f32;
    set.lt.and.f32.f64  f32, r0, r1, p0;
    cvt.f64.f32 r0, f32;
    set.gt.and.f32.f64  f32, r0, in1, p0;
    cvt.f64.f32 r0, f32;
    set.lt.or.f32.f64  f32, r0, 1.5, p0;
    cvt.f64.f32 r0, f32;
    set.lt.xor.f32.f64  f32, r0, -1.5, p0;


    //cvt{.frnd}{.ftz}{.sat}.dtype.atype d, a; // fp rounding
    //.frnd = { .rn, .rz, .rm, .rp };
    //.dtype = .atype = { .u8, .u16, .u32, .u64, .s8, .s16, .s32, .s64, .f16, .f32, .f64 };

    //cvt.rn.f32.f64 f0, r0;
    //cvt.rz.f32.f32 r0, r0;
    //cvt.rm.f32.f32 r0, r0;
    //cvt.rp.f32.f32 r0, r0;
    cvt.f64.f64 r0, r0;
    abs.f64 r0, r0;
    cvt.f64.f64 r0, r0;
    neg.f64 r0, r0;
    cvt.f64.f64 r0, r0;

    cvt.rn.f32.f64 f32, r0;
    cvt.f64.f32 r0, f32;
    cvt.rz.f32.f64 f32, r0;
    cvt.f64.f32 r0, f32;
    cvt.rm.f32.f64 f32, r0;
    cvt.f64.f32 r0, f32;
    cvt.rp.f32.f64 f32, r0;
    cvt.f64.f32 r0, f32;

    neg.f64 r0, r0;
    cvt.rni.s64.f64 s64, r0;
    cvt.rn.f64.s64 r0, s64;
    abs.f64 r0, r0;
    cvt.rni.u64.f64 u64, r0;
    cvt.rn.f64.u64 r0, u64;
    cvt.rni.s32.f64 s32, r0;
    cvt.rn.f64.s32 r0, s32;
    cvt.rni.u32.f64 u32, r0;
    cvt.rn.f64.u32 r0, u32;

    st.global.wb.f64 [out+4], r0;
    st.global.cg.f64 [out+4], r0;
    st.global.cs.f64 [out+8], r0;
    st.global.wt.f64 [out+12], r0;

    ret;
}

.visible .entry special2(
    .param .u32 .ptr.global.align 4 param_out32,
    .param .u32 param_u32
)
{
    .reg .u32 out32, in32;
    .reg .s32 var<2>;

    ld.param.u32 in32, [param_u32];
    ld.param.u32 out32, [param_out32];

    clz.b32 var0, in32;

    st.global.s32 [out32+0], var0;
    ret;
}

.visible .entry special3(
    .param .u32 .ptr.global.align 4 param_out32,
    .param .s16 param_s16,
    .param .s32 param_s32
)
{
    .reg .u32 out32;
    .reg .s16 var16;
    .reg .s32 var32;

    ld.param.s16 var16, [param_s16];
    ld.param.s32 var32, [param_s32];
    ld.param.u32 out32, [param_out32];

    //mad.wide.s16 var32, var16, var16, var32;
    mad.lo.s16 var16, var16, 13, var16;

    st.global.s16 [out32+0], var16;
    ret;
}

.visible .entry special(
    .param .u32 .ptr.global.align 4 param_out32,
    .param .u32 .ptr.global.align 8 param_out64
)
{
    .reg .u32 out32, out64;
    .reg .s32 var<28>;
    .reg .s64 dvar<3>;

    ld.param.u32 out32, [param_out32];
    ld.param.u32 out64, [param_out64];

    mov.s32 var0, %tid.x;
    mov.s32 var1, %tid.y;
    mov.s32 var2, %tid.z;

    mov.s32 var3, %ntid.x;
    mov.s32 var4, %ntid.y;
    mov.s32 var5, %ntid.z;

    mov.s32 var6, %laneid;
    mov.s32 var7, %warpid;
    mov.s32 var8, %nwarpid;

    mov.s32 var9, %ctaid.x;
    mov.s32 var10, %ctaid.y;
    mov.s32 var11, %ctaid.z;

    mov.s32 var12, %nctaid.x;
    mov.s32 var13, %nctaid.y;
    mov.s32 var14, %nctaid.z;

    mov.s32 var15, %smid;
    mov.s32 var16, %nsmid;
    mov.s32 var17, %gridid;

    mov.s32 var19, %lanemask_eq;
    mov.s32 var20, %lanemask_le;
    mov.s32 var21, %lanemask_lt;
    mov.s32 var22, %lanemask_ge;
    mov.s32 var23, %lanemask_gt;

    mov.s32 var24, %clock;
    mov.s32 var25, %globaltimer_lo;
    mov.s32 var26, %globaltimer_hi;

    mov.s64 dvar0, %clock64;
    mov.s64 dvar1, %globaltimer;

    add.s32 var27, var0,  var1;
    add.s32 var27, var27, var2;
    add.s32 var27, var27, var3;
    add.s32 var27, var27, var4;
    add.s32 var27, var27, var5;
    add.s32 var27, var27, var6;
    add.s32 var27, var27, var7;
    add.s32 var27, var27, var8;
    add.s32 var27, var27, var9;
    add.s32 var27, var27, var10;
    add.s32 var27, var27, var11;
    add.s32 var27, var27, var12;
    add.s32 var27, var27, var13;
    add.s32 var27, var27, var14;
    add.s32 var27, var27, var15;
    add.s32 var27, var27, var16;
    add.s32 var27, var27, var17;
    add.s32 var27, var27, var18;
    add.s32 var27, var27, var19;
    add.s32 var27, var27, var20;
    add.s32 var27, var27, var21;
    add.s32 var27, var27, var22;
    add.s32 var27, var27, var23;
    add.s32 var27, var27, var24;
    add.s32 var27, var27, var25;
    add.s32 var27, var27, var26;

    add.s64 dvar2, dvar0,  dvar1;

    st.global.s32 [out32+0], var27;
    st.global.s64 [out64+0], dvar2;
    ret;
}

.visible .entry logic(
    .param .u32 .ptr.global.align 4 param_out,
    .param .b16 param_b16,
    .param .u16 param_u16,
    .param .s16 param_s16,
    .param .b32 param_b32,
    .param .u32 param_u32,
    .param .s32 param_s32,
    .param .b64 param_b64,
    .param .s64 param_u64,
    .param .s64 param_s64,
    .param .u32 param_shift
)
{
    .reg .u32 out;
    .reg .b16 b16, rb16;
    .reg .u16 u16, ru16;
    .reg .s16 s16, rs16;
    .reg .b32 b32, rb32;
    .reg .u32 u32, ru32, shift;
    .reg .s32 s32, rs32;
    .reg .b64 b64, rb64;
    .reg .s64 u64, ru64;
    .reg .s64 s64, rs64;
    .reg .pred p<7>;

    ld.param.u32 out, [param_out];
    ld.param.u16 u16, [param_u16];
    ld.param.s16 s16, [param_s16];
    ld.param.u32 u32, [param_u32];
    ld.param.s32 s32, [param_s32];
    ld.param.s64 u64, [param_u64];
    ld.param.s64 s64, [param_s64];
    ld.param.u32 shift, [param_shift];

    //shr.type d, a, b;
    //.type = { .b16, .b32, .b64, .u16, .u32, .u64, .s16, .s32, .s64 };

    mad.lo.s32 rs32, s32, s32, s32;

    //and.pred p0,

    //neg.f32 r1, r0;
    setp.lt.s64 p0, u64, s64;
    //add.rn.f32 r0, r0, r0;
    setp.lt.and.s64 p1, u64, s64, p0;

    xor.pred p0, p0, p1;

    @p0 add.s64 rs64, s64, s64;

    //abs.f32 r1, r0;
    //setp.lt.and.f32 p0, r1, r0, p0;

    bfi.b32 ru32, u32, u32, 3, 7;
    bfi.b32 ru32, ru32, ru32, 7, shift;
    bfi.b32 ru32, ru32, ru32, ru32, ru32;

    shf.l.clamp.b32 ru32, ru32, ru32, u32;
    shf.r.clamp.b32 ru32, ru32, ru32, u32;

    shf.l.clamp.b32 ru32, ru32, ru32, 16;
    shf.l.wrap.b32 ru32, ru32, ru32, 16;

    shf.r.clamp.b32 ru32, ru32, ru32, 16;
    shf.r.wrap.b32 ru32, ru32, ru32, 16;

    popc.b32  rs32, s32;

    //prmt.b32{.mode} d, a, b, c; .mode = { .f4e, .b4e, .rc8, .ecl, .ecr, .rc16 };

    prmt.b32 ru32, ru32, s32, 7;
    prmt.b32 ru32, ru32, s32, u32;
    prmt.b32 ru32, ru32, s32, ru32;
    prmt.b32 ru32, ru32, ru32, ru32;
    prmt.b32.f4e ru32, ru32, s32, 7;
    prmt.b32.b4e ru32, ru32, s32, 7;
    prmt.b32.rc8 ru32, ru32, s32, 7;
    prmt.b32.ecl ru32, ru32, s32, 7;
    prmt.b32.ecr ru32, ru32, s32, 7;
    prmt.b32.rc16 ru32, ru32, s32, 7;

    //shfl.mode.b32 d[|p], a, b, c; .mode = { .up, .down, .bfly, .idx };
    shfl.up.b32 ru32, ru32, 0x0, 0x1111;
    shfl.down.b32 ru32, ru32, 0x0, 0x1111;
    shfl.bfly.b32 ru32, ru32, 0x0, 0x1111;
    shfl.idx.b32 ru32, ru32, 0x0, 0x1111;
    shfl.idx.b32 ru32|p0, ru32, 0x0, 0x1111;

    shfl.idx.b32 ru32, ru32, 0x0, u32;
    shfl.idx.b32 ru32, ru32, s32, 0x1111;
    shfl.idx.b32 ru32, ru32, s32, u32;

    bar.sync 0;
    bar.sync 1;
    bar.sync 1, 32;
    bar.arrive 0, 32;
    bar.arrive 1, 32;

    //bar.sync ru32;
    //bar.sync ru32, u32;
    //bar.arrive ru32, 32;
    //bar.arrive ru32, u32;

    //bar.red.popc.u32 ru32, u32, u32, p0;
    //bar.red.popc.u32 ru32, u32, u32, !p0;

    bar.red.popc.u32 ru32, 1, p0;
    bar.red.popc.u32 ru32, 1, 32, p0;

    //bar.red.and.pred p1, u32, u32, p0;
    //bar.red.or.pred p2, u32, u32, p1;

    bar.red.or.pred p3, 7, p2;
    bar.red.or.pred p4, 7, 32, p3;

    LABEL:
    vote.all.pred p1, p0;
    vote.any.pred p2, p1;
    vote.uni.pred p3, !p2;
    @p1 bra LABEL;
    @p2 bra.uni LABEL;
    //@p3 bra.uni ru32;

    vote.ballot.b32 u32, p3;
    add.s32 ru32, ru32, u32;

    brkpt;
    trap;

    @p0 st.global.u16 [out+0], ru16;
    membar.cta;
    @p1 st.global.s16 [out+2], rs16;
    membar.gl;
    @p2 st.global.u32 [out+4], ru32;
    membar.sys;
    @p3 st.global.s32 [out+8], rs32;
    @p4 st.global.u64 [out+16], ru64;
    st.global.s64 [out+24], rs64;
    ret;
}

.visible .entry memory(
    .param .u32 .ptr.global.align 1 param_in_u8,
    .param .u32 .ptr.global.align 1 param_in_s8,
    .param .u32 .ptr.global.align 2 param_in_u16,
    .param .u32 .ptr.global.align 2 param_in_s16,
    .param .u32 .ptr.global.align 4 param_in_u32,
    .param .u32 .ptr.global.align 4 param_in_s32,
    .param .u32 .ptr.global.align 4 param_in_f32,
    .param .u32 .ptr.global.align 8 param_in_u64,
    .param .u32 .ptr.global.align 8 param_in_s64,
    .param .u32 .ptr.global.align 8 param_in_f64,
    .param .u32 .ptr.global.align 1 param_out_u8,
    .param .u32 .ptr.global.align 1 param_out_s8,
    .param .u32 .ptr.global.align 2 param_out_u16,
    .param .u32 .ptr.global.align 2 param_out_s16,
    .param .u32 .ptr.global.align 4 param_out_u32,
    .param .u32 .ptr.global.align 4 param_out_s32,
    .param .u32 .ptr.global.align 4 param_out_f32,
    .param .u32 .ptr.global.align 8 param_out_u64,
    .param .u32 .ptr.global.align 8 param_out_s64,
    .param .u32 .ptr.global.align 8 param_out_f64,

    .param .u32 .ptr.global.align 2  param_in_v2u8,
    .param .u32 .ptr.global.align 2  param_in_v2s8,
    .param .u32 .ptr.global.align 4  param_in_v2u16,
    .param .u32 .ptr.global.align 4  param_in_v2s16,
    .param .u32 .ptr.global.align 8  param_in_v2u32,
    .param .u32 .ptr.global.align 8  param_in_v2s32,
    .param .u32 .ptr.global.align 8  param_in_v2f32,
    .param .u32 .ptr.global.align 16 param_in_v2u64,
    .param .u32 .ptr.global.align 16 param_in_v2s64,
    .param .u32 .ptr.global.align 16 param_in_v2f64,
    .param .u32 .ptr.global.align 2  param_out_v2u8,
    .param .u32 .ptr.global.align 2  param_out_v2s8,
    .param .u32 .ptr.global.align 4  param_out_v2u16,
    .param .u32 .ptr.global.align 4  param_out_v2s16,
    .param .u32 .ptr.global.align 8  param_out_v2u32,
    .param .u32 .ptr.global.align 8  param_out_v2s32,
    .param .u32 .ptr.global.align 8  param_out_v2f32,
    .param .u32 .ptr.global.align 16 param_out_v2u64,
    .param .u32 .ptr.global.align 16 param_out_v2s64,
    .param .u32 .ptr.global.align 16 param_out_v2f64,

    .param .u32 .ptr.global.align 4  param_in_v4u8,
    .param .u32 .ptr.global.align 4  param_in_v4s8,
    .param .u32 .ptr.global.align 8  param_in_v4u16,
    .param .u32 .ptr.global.align 8  param_in_v4s16,
    .param .u32 .ptr.global.align 16 param_in_v4u32,
    .param .u32 .ptr.global.align 16 param_in_v4s32,
    .param .u32 .ptr.global.align 16 param_in_v4f32,
    .param .u32 .ptr.global.align 4  param_out_v4u8,
    .param .u32 .ptr.global.align 4  param_out_v4s8,
    .param .u32 .ptr.global.align 8  param_out_v4u16,
    .param .u32 .ptr.global.align 8  param_out_v4s16,
    .param .u32 .ptr.global.align 16 param_out_v4u32,
    .param .u32 .ptr.global.align 16 param_out_v4s32,
    .param .u32 .ptr.global.align 16 param_out_v4f32
)
{
    .shared .align 16 .b8 shr16[512];
    .shared .align 8  .b8 shr8[512];
    .shared .align 4  .b8 shr4[512];
    .shared .align 2  .b8 shr2[512];
    .shared .align 1  .b8 shr1[512];

    .reg .u32 in_u8,  out_u8,  in_v2u8,  out_v2u8,  in_v4u8,  out_v4u8;
    .reg .u32 in_s8,  out_s8,  in_v2s8,  out_v2s8,  in_v4s8,  out_v4s8;
    .reg .u32 in_u16, out_u16, in_v2u16, out_v2u16, in_v4u16, out_v4u16;
    .reg .u32 in_s16, out_s16, in_v2s16, out_v2s16, in_v4s16, out_v4s16;
    .reg .u32 in_u32, out_u32, in_v2u32, out_v2u32, in_v4u32, out_v4u32;
    .reg .u32 in_s32, out_s32, in_v2s32, out_v2s32, in_v4s32, out_v4s32;
    .reg .u32 in_f32, out_f32, in_v2f32, out_v2f32, in_v4f32, out_v4f32;
    .reg .u32 in_u64, out_u64, in_v2u64, out_v2u64;
    .reg .u32 in_s64, out_s64, in_v2s64, out_v2s64;
    .reg .u32 in_f64, out_f64, in_v2f64, out_v2f64;

    .reg .u8  u8;
    .reg .s8  s8;
    .reg .u16 u16;
    .reg .s16 s16;
    .reg .u32 u32;
    .reg .s32 s32;
    .reg .f32 f32;
    .reg .u64 u64;
    .reg .s64 s64;
    .reg .f64 f64;

    .reg .v2 .u8  v2u8;
    .reg .v2 .s8  v2s8;
    .reg .v2 .u16 v2u16;
    .reg .v2 .s16 v2s16;
    .reg .v2 .u32 v2u32;
    .reg .v2 .s32 v2s32;
    .reg .v2 .f32 v2f32;
    .reg .v2 .u64 v2u64;
    .reg .v2 .s64 v2s64;
    .reg .v2 .f64 v2f64;

    .reg .v4 .u8  v4u8;
    .reg .v4 .s8  v4s8;
    .reg .v4 .u16 v4u16;
    .reg .v4 .s16 v4s16;
    .reg .v4 .u32 v4u32;
    .reg .v4 .s32 v4s32;
    .reg .v4 .f32 v4f32;

    .reg .pred p<7>;

    ld.param.u32 in_u8,  [param_in_u8];
    ld.param.u32 in_s8,  [param_in_s8];
    ld.param.u32 in_u16, [param_in_u16];
    ld.param.u32 in_s16, [param_in_s16];
    ld.param.u32 in_u32, [param_in_u32];
    ld.param.u32 in_s32, [param_in_s32];
    ld.param.u32 in_f32, [param_in_f32];
    ld.param.u32 in_u64, [param_in_u64];
    ld.param.u32 in_s64, [param_in_s64];
    ld.param.u32 in_f64, [param_in_f64];

    //isspacep.global  p0, in_u32;
    //cvta.to.global.u32 in_u8, in_u8;

    ld.param.u32 out_u8,  [param_out_u8];
    ld.param.u32 out_s8,  [param_out_s8];
    ld.param.u32 out_u16, [param_out_u16];
    ld.param.u32 out_s16, [param_out_s16];
    ld.param.u32 out_u32, [param_out_u32];
    ld.param.u32 out_s32, [param_out_s32];
    ld.param.u32 out_f32, [param_out_f32];
    ld.param.u32 out_u64, [param_out_u64];
    ld.param.u32 out_s64, [param_out_s64];
    ld.param.u32 out_f64, [param_out_f64];

    ld.param.u32 in_v2u8,  [param_in_v2u8];
    ld.param.u32 in_v2s8,  [param_in_v2s8];
    ld.param.u32 in_v2u16, [param_in_v2u16];
    ld.param.u32 in_v2s16, [param_in_v2s16];
    ld.param.u32 in_v2u32, [param_in_v2u32];
    ld.param.u32 in_v2s32, [param_in_v2s32];
    ld.param.u32 in_v2f32, [param_in_v2f32];
    ld.param.u32 in_v2u64, [param_in_v2u64];
    ld.param.u32 in_v2s64, [param_in_v2s64];
    ld.param.u32 in_v2f64, [param_in_v2f64];

    ld.param.u32 out_v2u8,  [param_out_v2u8];
    ld.param.u32 out_v2s8,  [param_out_v2s8];
    ld.param.u32 out_v2u16, [param_out_v2u16];
    ld.param.u32 out_v2s16, [param_out_v2s16];
    ld.param.u32 out_v2u32, [param_out_v2u32];
    ld.param.u32 out_v2s32, [param_out_v2s32];
    ld.param.u32 out_v2f32, [param_out_v2f32];
    ld.param.u32 out_v2u64, [param_out_v2u64];
    ld.param.u32 out_v2s64, [param_out_v2s64];
    ld.param.u32 out_v2f64, [param_out_v2f64];

    ld.param.u32 in_v4u8,  [param_in_v4u8];
    ld.param.u32 in_v4s8,  [param_in_v4s8];
    ld.param.u32 in_v4u16, [param_in_v4u16];
    ld.param.u32 in_v4s16, [param_in_v4s16];
    ld.param.u32 in_v4u32, [param_in_v4u32];
    ld.param.u32 in_v4s32, [param_in_v4s32];
    ld.param.u32 in_v4f32, [param_in_v4f32];

    ld.param.u32 out_v4u8,  [param_out_v4u8];
    ld.param.u32 out_v4s8,  [param_out_v4s8];
    ld.param.u32 out_v4u16, [param_out_v4u16];
    ld.param.u32 out_v4s16, [param_out_v4s16];
    ld.param.u32 out_v4u32, [param_out_v4u32];
    ld.param.u32 out_v4s32, [param_out_v4s32];
    ld.param.u32 out_v4f32, [param_out_v4f32];

    ld.global.u8  u8 , [in_u8 +0x1];
    ld.global.s8  s8 , [in_s8 +1];
    ld.global.u16 u16, [in_u16+2];
    ld.global.s16 s16, [in_s16+2];
    ld.global.u32 u32, [in_u32+4];
    ld.global.s32 s32, [in_s32+4];
    ld.global.f32 f32, [in_f32+4];
    ld.global.u64 u64, [in_u64+8];
    ld.global.s64 s64, [in_s64+8];
    ld.global.f64 f64, [in_f64+8];

    ld.global.v2.u8  v2u8 , [in_v2u8 +2 ];
    ld.global.v2.s8  v2s8 , [in_v2s8 +2 ];
    ld.global.v2.u16 v2u16, [in_v2u16+4 ];
    ld.global.v2.s16 v2s16, [in_v2s16+4 ];
    ld.global.v2.u32 v2u32, [in_v2u32+8 ];
    ld.global.v2.s32 v2s32, [in_v2s32+8 ];
    ld.global.nc.v2.f32 v2f32, [in_v2f32+8 ];
    ld.global.ca.nc.v2.u64 v2u64, [in_v2u64+16];
    ld.global.cg.nc.v2.s64 v2s64, [in_v2s64+16];
    ld.global.cs.nc.v2.f64 v2f64, [in_v2f64+16];

    ld.global.v4.u8  v4u8 , [in_v4u8 +4 ];
    ld.global.ca.v4.s8  v4s8 , [in_v4s8 +4 ];
    ld.global.cg.v4.u16 v4u16, [in_v4u16+8 ];
    ld.global.cs.v4.s16 v4s16, [in_v4s16+8 ];
    ld.global.lu.v4.u32 v4u32, [in_v4u32+16];
    ld.global.cv.v4.s32 v4s32, [in_v4s32+16];
    ld.global.v4.f32 v4f32, [in_v4f32+16];

    add.s16 v2s16.y, v2s16.x, v2s16.y;
    add.u16 v2u16.y, v2u16.x, v2u16.y;

    st.global.shared.u8     [shr1+1], u8 ;
    st.global.shared.s8     [shr1+1], s8 ;
    st.global.shared.u16    [shr2+2], u16;
    st.global.shared.s16    [shr2+2], s16;
    st.global.shared.u32    [shr4+4], u32;
    st.global.shared.s32    [shr4+4], s32;
    st.global.shared.f32    [shr4+4], f32;
    st.global.shared.u64    [shr8+8], u64;
    st.global.shared.s64    [shr8+8], s64;
    st.global.shared.f64    [shr8+8], f64;

    st.global.shared.v2.u8  [shr2 +2 ], v2u8 ;
    st.global.shared.v2.s8  [shr2 +2 ], v2s8 ;
    st.global.shared.v2.u16 [shr4 +4 ], v2u16;
    st.global.shared.v2.s16 [shr4 +4 ], v2s16;
    st.global.shared.v2.u32 [shr8 +8 ], v2u32;
    st.global.shared.v2.s32 [shr8 +8 ], v2s32;
    st.global.shared.v2.f32 [shr8 +8 ], v2f32;
    st.global.shared.v2.u64 [shr16+16], v2u64;
    st.global.shared.v2.s64 [shr16+16], v2s64;
    st.global.shared.v2.f64 [shr16+16], v2f64;

    st.global.shared.v4.u8  [shr4 +4 ], v4u8 ;
    st.global.shared.v4.s8  [shr4 +4 ], v4s8 ;
    st.global.shared.v4.u16 [shr8 +8 ], v4u16;
    st.global.shared.v4.s16 [shr8 +8 ], v4s16;
    st.global.shared.v4.u32 [shr16+16], v4u32;
    st.global.shared.v4.s32 [shr16+16], v4s32;
    st.global.shared.v4.f32 [shr16+16], v4f32;

    ld.shared.u8   u8 , [shr1+1];
    ld.shared.s8   s8 , [shr1+1];
    ld.shared.u16  u16, [shr2+2];
    ld.shared.s16  s16, [shr2+2];
    ld.shared.u32  u32, [shr4+4];
    ld.shared.s32  s32, [shr4+4];
    ld.shared.f32  f32, [shr4+4];
    ld.shared.u64  u64, [shr8+8];
    ld.shared.s64  s64, [shr8+8];
    ld.shared.f64  f64, [shr8+8];

    ld.shared.v2.u8  v2u8 , [shr2 +2 ];
    ld.shared.v2.s8  v2s8 , [shr2 +2 ];
    ld.shared.v2.u16 v2u16, [shr4 +4 ];
    ld.shared.v2.s16 v2s16, [shr4 +4 ];
    ld.shared.v2.u32 v2u32, [shr8 +8 ];
    ld.shared.v2.s32 v2s32, [shr8 +8 ];
    ld.shared.v2.f32 v2f32, [shr8 +8 ];
    ld.shared.v2.u64 v2u64, [shr16+16];
    ld.shared.v2.s64 v2s64, [shr16+16];
    ld.shared.v2.f64 v2f64, [shr16+16];

    ld.shared.v4.u8  v4u8 , [shr4 +4 ];
    ld.shared.v4.s8  v4s8 , [shr4 +4 ];
    ld.shared.v4.u16 v4u16, [shr8 +8 ];
    ld.shared.v4.s16 v4s16, [shr8 +8 ];
    ld.shared.v4.u32 v4u32, [shr16+16];
    ld.shared.v4.s32 v4s32, [shr16+16];
    ld.shared.v4.f32 v4f32, [shr16+16];

    //red{.space}.op.type [a], b;
    //atom{.space}.op.type  d, [a], b;
    //atom{.space}.op.type  d, [a], b, c;
    //.space = { .global, .shared };
    //.op = {
    //  .and, .or, .xor, // .b32, .b64
    //  .cas, .exch,     // .b32, .b64
    //  .add, // .u32, .s32, .f32, .u64
    //  .inc, .dec, // .u32 only
    //  .min, .max }; // .u32, .s32, .u64, .s64
    //.type = { .b32, .b64, .u32, .u64, .s32, .s64, .f32 };

    atom.shared.and.b32 u32, [shr4 +4 ], u32;
    atom.shared.or.b32  u32, [shr4 +4 ], u32;
    atom.shared.xor.b32 u32, [shr4 +4 ], u32;
    atom.shared.and.b64 u64, [shr8 +8 ], u64;
    atom.shared.or.b64  u64, [shr8 +8 ], u64;
    atom.shared.xor.b64 u64, [shr8 +8 ], u64;

    atom.shared.cas.b32  u32, [shr4 +4 ], u32, s32;
    atom.shared.exch.b32 u32, [shr4 +4 ], u32;
    atom.shared.cas.b64  u64, [shr8 +8 ], u64, s64;
    atom.shared.exch.b64 u64, [shr8 +8 ], u64;

    atom.shared.add.u32 u32, [shr4 +4 ], u32;
    atom.shared.add.s32 s32, [shr4 +4 ], s32;
    atom.shared.add.f32 f32, [shr4 +4 ], f32;
    atom.shared.add.u64 u64, [shr8 +8 ], u64;

    atom.shared.inc.u32 u32, [shr4 +4 ], u32;
    atom.shared.dec.u32 u32, [shr4 +4 ], u32;

    atom.shared.min.u32 u32, [shr4 +4 ], u32;
    atom.shared.min.s32 s32, [shr4 +4 ], s32;
    atom.shared.min.s64 s64, [shr8 +8 ], s64;
    atom.shared.min.u64 u64, [shr8 +8 ], u64;

    atom.shared.max.u32 u32, [shr4 +4 ], u32;
    atom.shared.max.s32 s32, [shr4 +4 ], s32;
    atom.shared.max.s64 s64, [shr8 +8 ], s64;
    atom.shared.max.u64 u64, [shr8 +8 ], u64;

    atom.global.and.b32 u32, [out_u32 +4 ], u32;
    atom.global.or.b32  u32, [out_u32 +4 ], u32;
    atom.global.xor.b32 u32, [out_u32 +4 ], u32;
    atom.global.and.b64 u64, [out_u64 +8 ], u64;
    atom.global.or.b64  u64, [out_u64 +8 ], u64;
    atom.global.xor.b64 u64, [out_u64 +8 ], u64;

    atom.global.cas.b32  u32, [out_u32 +4 ], u32, s32;
    atom.global.exch.b32 u32, [out_u32 +4 ], u32;
    atom.global.cas.b64  u64, [out_u64 +8 ], u64, s64;
    atom.global.exch.b64 u64, [out_u64 +8 ], u64;

    atom.global.add.u32 u32, [out_u32 +4 ], u32;
    atom.global.add.s32 s32, [out_s32 +4 ], s32;
    atom.global.add.f32 f32, [out_f32 +4 ], f32;
    atom.global.add.u64 u64, [out_u64 +8 ], u64;

    atom.global.inc.u32 u32, [out_u32 +4 ], u32;
    atom.global.dec.u32 u32, [out_u32 +4 ], u32;

    atom.global.min.u32 u32, [out_u32 +4 ], u32;
    atom.global.min.s32 s32, [out_s32 +4 ], s32;
    atom.global.min.s64 s64, [out_s64 +8 ], s64;
    atom.global.min.u64 u64, [out_u64 +8 ], u64;

    atom.global.max.u32 u32, [out_u32 +4 ], u32;
    atom.global.max.s32 s32, [out_s32 +4 ], s32;
    atom.global.max.s64 s64, [out_s64 +8 ], s64;
    atom.global.max.u64 u64, [out_u64 +8 ], u64;

    red.shared.and.b32 [shr4 +4 ], u32;
    red.shared.or.b32  [shr4 +4 ], u32;
    red.shared.xor.b32 [shr4 +4 ], u32;
    red.shared.and.b64 [shr8 +8 ], u64;
    red.shared.or.b64  [shr8 +8 ], u64;
    red.shared.xor.b64 [shr8 +8 ], u64;

    red.shared.add.u32 [shr4 +4 ], u32;
    red.shared.add.s32 [shr4 +4 ], s32;
    red.shared.add.f32 [shr4 +4 ], f32;
    red.shared.add.u64 [shr8 +8 ], u64;

    red.shared.inc.u32 [shr4 +4 ], u32;
    red.shared.dec.u32 [shr4 +4 ], u32;

    red.shared.min.u32 [shr4 +4 ], u32;
    red.shared.min.s32 [shr4 +4 ], s32;
    red.shared.min.s64 [shr8 +8 ], s64;
    red.shared.min.u64 [shr8 +8 ], u64;

    red.shared.max.u32 [shr4 +4 ], u32;
    red.shared.max.s32 [shr4 +4 ], s32;
    red.shared.max.s64 [shr8 +8 ], s64;
    red.shared.max.u64 [shr8 +8 ], u64;

    red.global.and.b32 [out_u32 +4 ], u32;
    red.global.or.b32  [out_u32 +4 ], u32;
    red.global.xor.b32 [out_u32 +4 ], u32;
    red.global.and.b64 [out_u64 +8 ], u64;
    red.global.or.b64  [out_u64 +8 ], u64;
    red.global.xor.b64 [out_u64 +8 ], u64;

    red.global.add.u32 [out_u32 +4 ], u32;
    red.global.add.s32 [out_s32 +4 ], s32;
    red.global.add.f32 [out_f32 +4 ], f32;
    red.global.add.u64 [out_u64 +8 ], u64;

    red.global.inc.u32 [out_u32 +4 ], u32;
    red.global.dec.u32 [out_u32 +4 ], u32;

    red.global.min.u32 [out_u32 +4 ], u32;
    red.global.min.s32 [out_s32 +4 ], s32;
    red.global.min.s64 [out_s64 +8 ], s64;
    red.global.min.u64 [out_u64 +8 ], u64;

    red.global.max.u32 [out_u32 +4 ], u32;
    red.global.max.s32 [out_s32 +4 ], s32;
    red.global.max.s64 [out_s64 +8 ], s64;
    red.global.max.u64 [out_u64 +8 ], u64;

    st.u8     [out_u8 +1], u8 ;
    st.s8     [out_s8 +1], s8 ;
    st.u16    [out_u16+2], u16;
    st.s16    [out_s16+2], s16;
    st.u32    [out_u32+4], u32;
    st.s32    [out_s32+4], s32;
    st.f32    [out_f32+4], f32;
    st.u64    [out_u64+8], u64;
    st.s64    [out_s64+8], s64;
    st.f64    [out_f64+8], f64;

    st.v2.u8  [out_v2u8 +2 ], v2u8 ;
    st.v2.s8  [out_v2s8 +2 ], v2s8 ;
    st.v2.u16 [out_v2u16+4 ], v2u16;
    st.v2.s16 [out_v2s16+4 ], v2s16;
    st.v2.u32 [out_v2u32+8 ], v2u32;
    st.v2.s32 [out_v2s32+8 ], v2s32;
    st.v2.f32 [out_v2f32+8 ], v2f32;
    st.v2.u64 [out_v2u64+16], v2u64;
    st.v2.s64 [out_v2s64+16], v2s64;
    st.v2.f64 [out_v2f64+16], v2f64;

    st.v4.u8  [out_v4u8 +4 ], v4u8 ;
    st.v4.s8  [out_v4s8 +4 ], v4s8 ;
    st.v4.u16 [out_v4u16+8 ], v4u16;
    st.wb.v4.s16 [out_v4s16+8 ], v4s16;
    st.cg.v4.u32 [out_v4u32+16], v4u32;
    st.cs.v4.s32 [out_v4s32+16], v4s32;
    st.wt.v4.f32 [out_v4f32+16], v4f32;


    ld.u8  u8 , [in_u8 +0x1];
    ld.s8  s8 , [in_s8 +1];
    ld.u16 u16, [in_u16+2];
    ld.s16 s16, [in_s16+2];
    ld.u32 u32, [in_u32+4];
    ld.s32 s32, [in_s32+4];
    ld.f32 f32, [in_f32+4];
    ld.u64 u64, [in_u64+8];
    ld.s64 s64, [in_s64+8];
    ld.f64 f64, [in_f64+8];

    ld.v2.u8  v2u8 , [in_v2u8 +2 ];
    ld.v2.s8  v2s8 , [in_v2s8 +2 ];
    ld.v2.u16 v2u16, [in_v2u16+4 ];
    ld.v2.s16 v2s16, [in_v2s16+4 ];
    ld.v2.u32 v2u32, [in_v2u32+8 ];
    ld.v2.s32 v2s32, [in_v2s32+8 ];
    ld.v2.f32 v2f32, [in_v2f32+8 ];
    ld.ca.v2.u64 v2u64, [in_v2u64+16];
    ld.cg.v2.s64 v2s64, [in_v2s64+16];
    ld.cs.v2.f64 v2f64, [in_v2f64+16];

    ld.v4.u8  v4u8 , [in_v4u8 +4 ];
    ld.ca.v4.s8  v4s8 , [in_v4s8 +4 ];
    ld.cg.v4.u16 v4u16, [in_v4u16+8 ];
    ld.cs.v4.s16 v4s16, [in_v4s16+8 ];
    ld.lu.v4.u32 v4u32, [in_v4u32+16];
    ld.cv.v4.s32 v4s32, [in_v4s32+16];
    ld.v4.f32 v4f32, [in_v4f32+16];

    st.u8     [out_u8 +1], u8 ;
    st.s8     [out_s8 +1], s8 ;
    st.u16    [out_u16+2], u16;
    st.s16    [out_s16+2], s16;
    st.u32    [out_u32+4], u32;
    st.s32    [out_s32+4], s32;
    st.f32    [out_f32+4], f32;
    st.u64    [out_u64+8], u64;
    st.s64    [out_s64+8], s64;
    st.f64    [out_f64+8], f64;

    st.v2.u8  [out_v2u8 +2 ], v2u8 ;
    st.v2.s8  [out_v2s8 +2 ], v2s8 ;
    st.v2.u16 [out_v2u16+4 ], v2u16;
    st.v2.s16 [out_v2s16+4 ], v2s16;
    st.v2.u32 [out_v2u32+8 ], v2u32;
    st.v2.s32 [out_v2s32+8 ], v2s32;
    st.v2.f32 [out_v2f32+8 ], v2f32;
    st.v2.u64 [out_v2u64+16], v2u64;
    st.v2.s64 [out_v2s64+16], v2s64;
    st.v2.f64 [out_v2f64+16], v2f64;

    st.v4.u8  [out_v4u8 +4 ], v4u8 ;
    st.v4.s8  [out_v4s8 +4 ], v4s8 ;
    st.v4.u16 [out_v4u16+8 ], v4u16;
    st.wb.v4.s16 [out_v4s16+8 ], v4s16;
    st.cg.v4.u32 [out_v4u32+16], v4u32;
    st.cs.v4.s32 [out_v4s32+16], v4s32;
    st.wt.v4.f32 [out_v4f32+16], v4f32;

    ret;
}


.visible .entry integer(
    .param .u32 .ptr.global.align 4 param_out,
    .param .s32 param_s32_1,
    .param .s32 param_s32_2,
    .param .s32 param_s32_3,
    .param .s32 param_s32_4,
    .param .s32 param_s32_5,
    .param .s32 param_s32_6
)
{
    .reg .u32 out;
    .reg .s32 s32_1, rs32_1;
    .reg .s32 s32_2, rs32_2;
    .reg .s32 s32_3, rs32_3;
    .reg .s32 s32_4, rs32_4;
    .reg .s32 s32_5, rs32_5;
    .reg .s32 s32_6, rs32_6;
    .reg .pred p<7>;

    ld.param.u32 out,   [param_out];
    ld.param.s32 s32_1, [param_s32_1];
    ld.param.s32 s32_2, [param_s32_2];
    ld.param.s32 s32_3, [param_s32_3];
    ld.param.s32 s32_4, [param_s32_4];
    ld.param.s32 s32_5, [param_s32_5];
    ld.param.s32 s32_6, [param_s32_6];

    mad.lo.s32 rs32_1, s32_1, s32_1, s32_1;
    mad.lo.s32 rs32_2, s32_2, s32_1, s32_2;
    mad.lo.s32 rs32_3, s32_3, s32_1, s32_3;
    mad.lo.s32 rs32_4, s32_4, s32_4, s32_4;
    mad.lo.s32 rs32_5, s32_5, s32_5, s32_5;
    mad.lo.s32 rs32_6, s32_6, s32_6, s32_6;

    mad.lo.s32 rs32_1, rs32_1, rs32_1, rs32_1;
    mad.lo.s32 rs32_2, rs32_2, rs32_2, rs32_2;
    mad.lo.s32 rs32_3, rs32_3, rs32_3, rs32_3;
    mad.lo.s32 rs32_4, rs32_4, rs32_4, rs32_4;
    mad.lo.s32 rs32_5, rs32_5, rs32_5, rs32_5;
    mad.lo.s32 rs32_6, rs32_6, rs32_6, rs32_6;

    mad.lo.s32 rs32_1, rs32_1, 4, rs32_1;
    mad.lo.s32 rs32_2, rs32_2, 4, rs32_2;
    mad.lo.s32 rs32_3, rs32_3, 4, rs32_3;
    mad.lo.s32 rs32_4, rs32_4, 4, rs32_4;
    mad.lo.s32 rs32_5, rs32_5, 4, rs32_5;
    mad.lo.s32 rs32_6, rs32_6, 4, rs32_6;

    mad.lo.s32 rs32_1, rs32_1, rs32_1, rs32_1;
    mad.lo.s32 rs32_2, rs32_2, rs32_2, rs32_2;
    mad.lo.s32 rs32_3, rs32_3, rs32_3, rs32_3;
    mad.lo.s32 rs32_4, rs32_4, rs32_4, rs32_4;
    mad.lo.s32 rs32_5, rs32_5, rs32_5, rs32_5;
    mad.lo.s32 rs32_6, rs32_6, rs32_6, rs32_6;

    add.s32 rs32_5, rs32_5, rs32_6;
    add.s32 rs32_4, rs32_4, rs32_5;
    add.s32 rs32_3, rs32_3, rs32_4;
    add.s32 rs32_2, rs32_2, rs32_3;
    add.s32 rs32_1, rs32_1, rs32_2;

    st.s32 [out], rs32_1;
}

.visible .entry icompare(
    .param .u32 .ptr.global.align 4 param_out,
    .param .s32 param_s32_1,
    .param .s32 param_s32_2,
    .param .s32 param_s32_3,
    .param .s32 param_s32_4,
    .param .s32 param_s32_5,
    .param .s32 param_s32_6
)
{
    .reg .u32 out;
    .reg .s32 s32_1, rs32_1, t0, t1, t2, t3, t4, t5;
    .reg .s32 s32_2, rs32_2;
    .reg .s32 s32_3, rs32_3;
    .reg .s32 s32_4, rs32_4;
    .reg .s32 s32_5, rs32_5;
    .reg .s32 s32_6, rs32_6;
    .reg .pred p<7>;

    ld.param.u32 out,   [param_out];
    ld.param.s32 s32_1, [param_s32_1];
    ld.param.s32 s32_2, [param_s32_2];
    ld.param.s32 s32_3, [param_s32_3];
    ld.param.s32 s32_4, [param_s32_4];
    ld.param.s32 s32_5, [param_s32_5];
    ld.param.s32 s32_6, [param_s32_6];

    add.s32 t0, s32_1, s32_1;
    add.s32 t1, t0, s32_2;
    add.s32 t2, t1, s32_3;
    add.s32 t3, t2, s32_4;
    add.s32 t4, t3, s32_5;
    add.s32 t5, t4, s32_6;

    setp.lt.s32 p6, t5, t4;
    setp.lt.and.s32 p0, t0, t1, p6;
    setp.lt.and.s32 p1, t1, t0, p0;
    setp.lt.and.s32 p2, t2, t0, p1;
    setp.lt.and.s32 p3, t3, t0, p2;
    setp.lt.and.s32 p4, t4, t0, p3;
    setp.lt.and.s32 p5, t5, t0, p4;

    //@p5 add.s32 rs32_5, rs32_5, rs32_6;
    //@p4 add.s32 rs32_4, rs32_4, rs32_5;
    //@p3 add.s32 rs32_3, rs32_3, rs32_4;
    //@p2 add.s32 rs32_2, rs32_2, rs32_3;
    //@p1 add.s32 rs32_1, rs32_1, rs32_2;
    @p5 add.s32 rs32_1, t4, t5;

    st.s32 [out], rs32_1;
}