# Kernel: sgemm_kernel_128
#
# SharedSize: 16384
# Params(8):
#   0:0x140:4:4 param_C,
#   1:0x144:4:0 param_m,
#   2:0x148:4:0 param_n,
#   3:0x14c:4:0 param_k,
#   4:0x150:4:0 param_lda,
#   5:0x154:4:0 param_ldb,
#   6:0x158:4:0 param_ldc
#   7:0x15c:4:4 param_D // for diagnostic printf output
#
# Globals:
#   c[0x0][0x160]: texA (the value is 1)
#   c[0x0][0x164]: texB (the value is 0)

<REGISTER_MAPPING>

    // 64 C maxtrix output registers.
    // Use special mapping to avoid register bank conflicts between these registers and the blocking registers.
     3, 2,11,10,19,18,27,26 : cx00y<00-03|64-67>
     7, 6,15,14,23,22,31,30 : cx01y<00-03|64-67>
     1, 0, 9, 8,17,16,25,24 : cx02y<00-03|64-67>
     5, 4,13,12,21,20,29,28 : cx03y<00-03|64-67>
    35,34,43,42,51,50,59,58 : cx64y<00-03|64-67>
    39,38,47,46,55,54,63,62 : cx65y<00-03|64-67>
    33,32,41,40,49,48,57,56 : cx66y<00-03|64-67>
    37,36,45,44,53,52,61,60 : cx67y<00-03|64-67>

    // Double buffered register blocking used in vector loads.
    // Any bank conflicts that we can't avoid in these registers we can hide with .reuse flags
    64-79   : j0Ax<00-03|64-67>, j0By<00-03|64-67>
    80-95   : j1Ax<00-03|64-67>, j1By<00-03|64-67>

    // The offset we store our zero value for initializing C
    // Reuse a register from the second blocking registers
    80      : zOffset
    // Aliases for the C registers we use for initializing C (used as vectors)
    0-63    : cz<00-63>

    // Registers to load A or B
    96-103  : loadX<0-7>

    // Key state registers for main loop and some we reuse for outputing C
    104-127 : track<0-7>, tex, end, ldx8, writeS, readAs, readBs, tid, tid31, tid96, tid128, bx, by

    // Temporary registers to calculate the state registers. Reuse the C output registers.
    // These can be dynamically allocated (~) in the available registger space to elimiate any register bank conflicts.
    0-63    ~ blk, ldx, ldx2, ldx4, k, tid1, tid4, tid7, xmad<0-1>

    // Registers to store the results back to global memory. Reuse any register not needed after the main loop.
    // Statically allocate cs0-7 because they're vector reegisters
    64-71   : cs<0-7>
    // dynamically allocated (~)
    72-115  ~ cy<00|04|08|12>, Cy<00|04|08|12>, m, n, ldc, ldc1, ldc4, ldc8, ldc60, writeCs, readCs, cx, ci, xmad2, D

</REGISTER_MAPPING>

// Scheduler doesn't handle the dependency flags yet,
// so move these first instructions outside the block that's auto scheduled
--:-:1:-:1      S2R tid, SR_TID.X;   // Set Dep 1
--:-:2:-:1      S2R bx,  SR_CTAID.X; // Set Dep 2
--:-:3:-:1      S2R by,  SR_CTAID.Y; // Set Dep 3

// Instructions in a SCHEDULE_BLOCK are automatically reordered and appropriately stalled for simple dependancies
// Memory dependencies are left up to the auther to deal with manually for now.
<SCHEDULE_BLOCK>

// First 128 threads load A to shared, 2nd 128 loads B to shared
// Note this technique is not possible in cuda or ptx as there's no way to
// efficiently specify a warp-uniform predicate for a memory op.
// Compile sgemm.cu and inspect the sass to see what I'm talking about.

// blk = tid >= 128 ? by   : bx;
// ldx = tid >= 128 ? ldb  : lda;
// tex = tid >= 128 ? texB : texA;
01:-:-:Y:1      ISETP.GE.AND P0, PT, tid, 128, PT; // Wait Dep 1
06:-:-:-:1      SEL blk, by, bx, P0;               // Wait Dep 2 & 3
--:-:-:-:1 @!P0 MOV ldx, c[0x0][0x150];
--:-:-:-:1  @P0 MOV ldx, c[0x0][0x154];
--:-:-:-:1 @!P0 MOV32I tex, 0x80000001; // texA
--:-:-:-:1  @P0 MOV32I tex, 0x80000000; // texB

// Initialize the portion of shared we use to zero our C registers
// Give each warp its own address to write to.
// All threads write to the same address, but we don't care because only one needs to take.
--:-:-:-:1      LOP.AND zOffset, tid, -32;
--:-:-:-:1      STS.128 [zOffset + 4x<2048>], RZ;

// tid4   = (tid >> 5) & 3
// tid31  = tid & 31
// tid96  = tid & 96
// tid128 = tid & 128
--:-:-:-:1      BFE.U32 tid4,   tid, 0x205; // 2 bits at position 5
--:-:-:-:1      LOP.AND tid31,  tid, 31;
--:-:-:-:1      LOP.AND tid96,  tid, 96;
--:-:-:-:1      LOP.AND tid128, tid, 128;

// ldx4  = ldx * 4;
// ldx8  = ldx * 8;
--:-:-:-:1      SHL  ldx4, ldx,  2;
--:-:-:-:1      IADD ldx8, ldx4, ldx4;

// track0 = blk*128 + tid31 + (ldx * tid4)
--:-:-:-:1      ISCADD  track0, blk, tid31, 7;
--:-:-:-:1      XMAD.LO track0, ldx, tid4,  track0, xmad0; // XMAD.LO is a macro that is expanded out into the 3 XMADs

// Setup 8 independent track vars so we don't need to synchronize between texture loads
--:-:-:-:1      IADD track1, track0, 32;
--:-:-:-:1      IADD track2, track0, 64;
--:-:-:-:1      IADD track3, track0, 96;
--:-:-:-:1      IADD track4, track0, ldx4;
--:-:-:-:1      IADD track5, track1, ldx4;
--:-:-:-:1      IADD track6, track2, ldx4;
--:-:-:-:1      IADD track7, track3, ldx4;

// writeS = (tid31 + tid4*128) * 4
--:-:-:-:1      ISCADD writeS, tid4,   tid31, 7;
--:-:-:-:1      SHL    writeS, writeS, 2;

// writeS += 4096 if tid >= 128
--:-:-:-:1  @P0 IADD   writeS, writeS, 4x<1024>;

// int end = track0 + (k-8)*ldx;
--:-:-:-:1      MOV k, c[0x0][0x14c];
--:-:-:-:1      IADD k, k, -8;
--:-:-:-:1      XMAD.LO end, k, ldx, track0, xmad1;

// readAs and readBs are carefully constructed to avoid any bank conflicts while loading from shared
// readAs = ((tid128 >> 4) | ((tid >> 1) & 7)) << 4;
--:-:-:-:1      BFE.U32 tid7,   tid,    0x301; // 3 bits at position 1
--:-:-:-:1      SHR.U32 readAs, tid128, 4;
--:-:-:-:1      LOP.OR  readAs, readAs, tid7;
--:-:-:-:1      SHL     readAs, readAs, 4;

// readBs  = (((tid & 0x70) >> 3) | (tid & 1)) << 4 + 4096;
--:-:-:-:1      LOP.AND tid1,   tid,    1;
--:-:-:-:1      LOP.AND readBs, tid,    0x70;
--:-:-:-:1      SHR.U32 readBs, readBs, 3;
--:-:-:-:1      LOP.OR  readBs, readBs, tid1;
--:-:-:-:1      ISCADD  readBs, readBs, 4x<1024>, 4;

// Preload the first 8 lines from texture memory
// Break this up into two barriers so we can load 4 at a time along a single cache line.
// Keep these instructions in this order (but allow others to interleave).
// Normally the scheduler tries to preserve source order by default, but this demonstrates how you enforce
// an ordering if you need to.
<ORDERED>
--:-:-:-:1      TLD.B.LZ.T loadX0, track0, tex, 0x0, 1D, 0x1;
--:-:-:-:1      TLD.B.LZ.T loadX1, track1, tex, 0x0, 1D, 0x1;
--:-:-:-:1      TLD.B.LZ.T loadX2, track2, tex, 0x0, 1D, 0x1;
--:-:1:-:1      TLD.B.LZ.P loadX3, track3, tex, 0x0, 1D, 0x1; // Set Dep 1
--:-:-:-:1      TLD.B.LZ.T loadX4, track4, tex, 0x0, 1D, 0x1;
--:-:-:-:1      TLD.B.LZ.T loadX5, track5, tex, 0x0, 1D, 0x1;
--:-:-:-:1      TLD.B.LZ.T loadX6, track6, tex, 0x0, 1D, 0x1;
--:-:2:-:1      TLD.B.LZ.P loadX7, track7, tex, 0x0, 1D, 0x1; // Set Dep 2
</ORDERED>

</SCHEDULE_BLOCK>

// These instuctions need to occur after the textures load so put them in a new block
// that starts with a dependency barrier wait.
<SCHEDULE_BLOCK>

01:-:-:-:1      STS [writeS + 4x<000 + 00>], loadX0; // Wait Dep 1
--:-:-:-:1      STS [writeS + 4x<000 + 32>], loadX1;
--:-:-:-:1      STS [writeS + 4x<000 + 64>], loadX2;
--:-:-:-:1      STS [writeS + 4x<000 + 96>], loadX3;
02:-:-:-:1      STS [writeS + 4x<512 + 00>], loadX4; // Wait Dep 2
--:-:-:-:1      STS [writeS + 4x<512 + 32>], loadX5;
--:-:-:-:1      STS [writeS + 4x<512 + 64>], loadX6;
--:-:-:-:1      STS [writeS + 4x<512 + 96>], loadX7;

// Increment tracks after the loads are complete to avoid needing write-after-read dependencies
--:-:-:-:1      IADD track0, track0, ldx8;
--:-:-:-:1      IADD track1, track1, ldx8;
--:-:-:-:1      IADD track2, track2, ldx8;
--:-:-:-:1      IADD track3, track3, ldx8;
--:-:-:-:1      IADD track4, track4, ldx8;
--:-:-:-:1      IADD track5, track5, ldx8;
--:-:-:-:1      IADD track6, track6, ldx8;
--:-:-:-:1      IADD track7, track7, ldx8;

// Wait for all threads to finish loading shared
--:-:-:-:5      BAR.SYNC 0x0;

</SCHEDULE_BLOCK>

// Initialize C registeres to zero
// Using LDS.U.128 is a neat trick to save a few clock cyles
// (when you have enough warps to hide the latency.)
<CODE>
    return join '', map sprintf("--:-:-:-:1      LDS.U.128 cz%02d, [zOffset + 4x<2048>];\n", $_ * 4), 0..15;
</CODE>

// The next store to shared goes to high area.
// Having 2 share buffers allows us to eliminate a bar.sync in the main loop.
// This way we don't have to wait for all threads to arrive before writing fresh data to shared.
// Other threads can continue reading from the last batch while the new data is being written.
--:-:-:-:0      LOP.XOR writeS, writeS, 4x<2048>;

// Preload the fist lines of A and B from shared
--:-:-:-:1      LDS.U.128 j0Ax00, [readAs + 4x<0*128 + 00>];
--:-:-:-:1      LDS.U.128 j0By00, [readBs + 4x<0*128 + 00>];
--:-:-:-:1      LDS.U.128 j0Ax64, [readAs + 4x<0*128 + 64>];
--:-:1:-:1      LDS.U.128 j0By64, [readBs + 4x<0*128 + 64>]; // Set Dep 1


// The main loop
// While calculating the first line, load in the next line from shared.
// Shared memory stores enough to do this 8 times per loop.
// Also pull in the next block of memory from global and store it to shared.

// Efficiency:
// ffma: 512
// lds:  32 dual issued
// sts:  8  dual issued
// tex:  8  dual issued
// add:  8
// xor:  3
// setp: 1
// bar:  1  dual issued
// bra:  1  dual issued
// Total: 524 (512/524 = 97.7% FFMA)

LOOP:

// Loop end condition
--:-:-:-:1      ISETP.LE.AND P0, PT, track0, end, PT;

<CODE>

    # We eliminated bank conflicts with our C registers and the blocking registers,
    # but there are still 16 bank conflicts between the blocking registers themselves.
    # By ordering the FFMA's in a specific pattern we can completely hide those conflicts
    # behind register reuse.  This pattern also maximizes that reuse and minimizes the bandwidth
    # out of the register bank, thereby reducing power consumption and allowing the chip to
    # stay at a higher sustained clock speed.
    my @cOrder = qw(
        x02y00 x00y00 x00y01 x02y01 x01y00 x03y00 x01y01 x03y01
        x64y00 x66y00 x64y01 x66y01 x65y00 x67y00 x65y01 x67y01
        x65y02 x67y02 x65y03 x67y03 x64y02 x66y02 x64y03 x66y03
        x01y02 x03y02 x01y03 x03y03 x00y02 x02y02 x00y03 x02y03
        x00y64 x02y64 x00y65 x02y65 x01y64 x03y64 x01y65 x03y65
        x64y64 x66y64 x64y65 x66y65 x65y64 x67y64 x65y65 x67y65
        x65y66 x67y66 x65y67 x67y67 x64y66 x66y66 x64y67 x66y67
        x01y66 x03y66 x01y67 x03y67 x00y66 x02y66 x00y67 x02y67
    );

    my %insert =
    (
        # Don't start the first TLD before 12 to let ISETP to write P0
        # These global reads and shared writes we put exactly in the middle of the LDS ops
        # This is to not overwhelm the memory units with instructions (and because these were tested faster here).
        j0c29 => "--:-:-:-:1  \@P0 TLD.B.LZ.T loadX0, track0, tex, 0x0, 1D, 0x1;\n",
        j0c31 => "--:-:-:-:1  \@P0 TLD.B.LZ.T loadX1, track1, tex, 0x0, 1D, 0x1;\n",
        j0c33 => "--:-:-:-:1  \@P0 TLD.B.LZ.T loadX2, track2, tex, 0x0, 1D, 0x1;\n",
        j0c35 => "--:-:2:-:1  \@P0 TLD.B.LZ.P loadX3, track3, tex, 0x0, 1D, 0x1; // Set Dep 2\n",

        j1c29 => "--:-:-:-:1  \@P0 TLD.B.LZ.T loadX4, track4, tex, 0x0, 1D, 0x1;\n",
        j1c31 => "--:-:-:-:1  \@P0 TLD.B.LZ.T loadX5, track5, tex, 0x0, 1D, 0x1;\n",
        j1c33 => "--:-:-:-:1  \@P0 TLD.B.LZ.T loadX6, track6, tex, 0x0, 1D, 0x1;\n",
        j1c35 => "--:-:3:-:1  \@P0 TLD.B.LZ.P loadX7, track7, tex, 0x0, 1D, 0x1; // Set Dep 3\n",

        j5c29 => "02:-:-:-:1  \@P0 STS [writeS + 4x<000 + 00>], loadX0;       // Wait Dep 2\n",
        j5c31 => "--:-:-:-:1  \@P0 STS [writeS + 4x<000 + 32>], loadX1;\n",
        j5c33 => "--:-:-:-:1  \@P0 STS [writeS + 4x<000 + 64>], loadX2;\n",
        j5c35 => "--:-:-:-:1  \@P0 STS [writeS + 4x<000 + 96>], loadX3;\n",

        j6c29 => "04:-:-:-:1  \@P0 STS [writeS + 4x<512 + 00>], loadX4;       // Wait Dep 3\n",
        j6c31 => "--:-:-:-:1  \@P0 STS [writeS + 4x<512 + 32>], loadX5;\n",
        j6c33 => "--:-:-:-:1  \@P0 STS [writeS + 4x<512 + 64>], loadX6;\n",
        j6c35 => "--:-:-:-:1  \@P0 STS [writeS + 4x<512 + 96>], loadX7;\n",

        # We need one barrier in the main loop after writing shared memory.
        # Note, BAR.SYNCs do not sync memory read access automatically, you still need to flag the barriers (writes are sync'd).
        # After the BAR, swap our share buffer location.  We don't need an additional barrier because of these swaps.
        # Note, this doubles our shared memory usage but this kernel's occupancy is entirely bound by registers.
        # LOP.XOR readAs needs to be 4 clocks prior to the LDS.U.128 for readAs (but push this as far down as possible)
        j6c62 =>
                "01:-:-:-:5  \@P0 BAR.SYNC 0x0; // Wait Dep 1\n" .
                "--:-:-:-:1  \@P0 LOP.XOR readAs, readAs, 4x<2048>;\n" .
                "--:-:-:-:1  \@P0 LOP.XOR readBs, readBs, 4x<2048>;\n" .
                "--:-:-:-:1  \@P0 LOP.XOR writeS, writeS, 4x<2048>;\n",

        # Note having 8 IADDs slightly hits our FFMA performance (8/520 = 1.5%), but TLD doesn't take an offset.
        # LDG.CI doesn't have this issue, but doesn't give you the nice features of texture loads:
        #   -Boundry Clamping:  simplifies our matrix load logic so we don't need to worry about loading out of bounds
        #   -Normalized Floats: if we don't need full 32 bits of precision we could store our matrices using 16 or 8 bit values
        j7c63 =>
                "--:-:-:-:1  \@P0 IADD track0, track0, ldx8;\n" .
                "--:-:-:-:1  \@P0 IADD track1, track1, ldx8;\n" .
                "--:-:-:-:1  \@P0 IADD track2, track2, ldx8;\n" .
                "--:-:-:-:1  \@P0 IADD track3, track3, ldx8;\n" .
                "--:-:-:-:1  \@P0 IADD track4, track4, ldx8;\n" .
                "--:-:-:-:1  \@P0 IADD track5, track5, ldx8;\n" .
                "--:-:-:-:1  \@P0 IADD track6, track6, ldx8;\n" .
                "--:-:-:-:0  \@P0 IADD track7, track7, ldx8;\n" .
                "--:-:-:Y:5  \@P0 BRA LOOP;\n",
    );

    my $out;
    # We unroll our main loop 8 iterations
    # This gives ample time for our textures to load.
    # We could unroll further but that increases the size of the program and increase instruction fetch latency on the branches.
    # We'd also need more shared memory to buffer the lines and more registers to store texture loads.
    # With metaprogramming you could parameterize all this and autotune this implementation for your matrix sizes.
    foreach my $j (0 .. 7)
    {
        my $odd      = $j & 1;
        my $nOdd     = !$odd + 0;
        # Our rolling blocking registers stay one load ahead off the FFMA's (rs: read share)
        my $rsOffset = ($j + 1) % 8;
        # No need to load on last loop iteration
        my $rsPred   = $j == 7 ? '@P0' : '   ';

        # You can experiment here with different vector load sizes
        my $vec = 128;

        if ($vec == 128)
        {
            # Roll up our LDS ops here to keep them easier to manage and tune
            $insert{"j${j}c0"} = sprintf "--:-:-:-:1  %s LDS.U.128 j%dAx00, [readAs + 4x<%d*128 + 00>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c2"} = sprintf "--:-:-:-:1  %s LDS.U.128 j%dBy00, [readBs + 4x<%d*128 + 00>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c4"} = sprintf "--:-:-:-:1  %s LDS.U.128 j%dAx64, [readAs + 4x<%d*128 + 64>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c6"} = sprintf "--:-:1:-:1  %s LDS.U.128 j%dBy64, [readBs + 4x<%d*128 + 64>]; // Set Dep 1\n", $rsPred, $nOdd, $rsOffset;
        }
        elsif ($vec == 64)
        {
            # LDS.128 runs about 40 Gflops faster than LDS.64 (GM107).  Not a huge difference since our latencies are so well hidden.
            # I think LDS.128 is implemented internally as a pair of LDS.64 ops which could be another reason for the comparable performance.
            # I think the big benefit with 128 is being able to issue all our LDS ops earlier, allowing more FFMA's prior to reading out the results.
            $insert{"j${j}c0"}  = sprintf "--:-:-:-:1  %s LDS.U.64 j%dAx00, [readAs + 4x<%d*128 + 00>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c2"}  = sprintf "--:-:-:-:1  %s LDS.U.64 j%dAx02, [readAs + 4x<%d*128 + 02>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c4"}  = sprintf "--:-:-:-:1  %s LDS.U.64 j%dBy00, [readBs + 4x<%d*128 + 00>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c6"}  = sprintf "--:-:-:-:1  %s LDS.U.64 j%dBy02, [readBs + 4x<%d*128 + 02>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c8"}  = sprintf "--:-:-:-:1  %s LDS.U.64 j%dAx64, [readAs + 4x<%d*128 + 64>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c10"} = sprintf "--:-:-:-:1  %s LDS.U.64 j%dAx66, [readAs + 4x<%d*128 + 66>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c12"} = sprintf "--:-:-:-:1  %s LDS.U.64 j%dBy64, [readBs + 4x<%d*128 + 64>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c14"} = sprintf "--:-:1:-:1  %s LDS.U.64 j%dBy66, [readBs + 4x<%d*128 + 66>]; // Set Dep 1\n", $rsPred, $nOdd, $rsOffset;
        }
        else
        {
            # This one drops performance by about 200 Gflops.  So you want to at least use LDS.64 if you can.
            $insert{"j${j}c0"}  = sprintf "--:-:-:-:1  %s LDS j%dAx00, [readAs + 4x<%d*128 + 00>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c1"}  = sprintf "--:-:-:-:1  %s LDS j%dAx01, [readAs + 4x<%d*128 + 01>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c2"}  = sprintf "--:-:-:-:1  %s LDS j%dAx02, [readAs + 4x<%d*128 + 02>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c3"}  = sprintf "--:-:-:-:1  %s LDS j%dAx03, [readAs + 4x<%d*128 + 03>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c4"}  = sprintf "--:-:-:-:1  %s LDS j%dBy00, [readBs + 4x<%d*128 + 00>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c5"}  = sprintf "--:-:-:-:1  %s LDS j%dBy01, [readBs + 4x<%d*128 + 01>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c6"}  = sprintf "--:-:-:-:1  %s LDS j%dBy02, [readBs + 4x<%d*128 + 02>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c7"}  = sprintf "--:-:-:-:1  %s LDS j%dBy03, [readBs + 4x<%d*128 + 03>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c8"}  = sprintf "--:-:-:-:1  %s LDS j%dAx64, [readAs + 4x<%d*128 + 64>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c9"}  = sprintf "--:-:-:-:1  %s LDS j%dAx65, [readAs + 4x<%d*128 + 65>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c10"} = sprintf "--:-:-:-:1  %s LDS j%dAx66, [readAs + 4x<%d*128 + 66>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c11"} = sprintf "--:-:-:-:1  %s LDS j%dAx67, [readAs + 4x<%d*128 + 67>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c12"} = sprintf "--:-:-:-:1  %s LDS j%dBy64, [readBs + 4x<%d*128 + 64>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c13"} = sprintf "--:-:-:-:1  %s LDS j%dBy65, [readBs + 4x<%d*128 + 65>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c14"} = sprintf "--:-:1:-:1  %s LDS j%dBy66, [readBs + 4x<%d*128 + 66>];\n", $rsPred, $nOdd, $rsOffset;
            $insert{"j${j}c15"} = sprintf "--:-:1:-:1  %s LDS j%dBy67, [readBs + 4x<%d*128 + 67>]; // Set Dep 1\n", $rsPred, $nOdd, $rsOffset;
        }
        foreach my $c (0 .. 63)
        {
            my ($x,$y) = $cOrder[$c] =~ /^(x\d+)(y\d+)/;

            # Grab an instruction for insertion if one exists for this j and c combination
            my $ins    = $insert{"j${j}c$c"} || '';

            # Scatter some yields in there to better balance the workload and reduce sync stalls
            # Don't pair a yeild with the dual issued ffmas as that kills performance for some reason
            my $yield  = $c == 28 ? 'Y' : '-';

            # The first FFMA needs to wait on the prior loop's LDS.U.128 ops to finish (except if the barrier does the wait for us)
            my ($waitDB, $comment) = $c == 0 && $j < 7 ? ('01', ' // Wait Dep 1') : ('--','');

            # Dual issue these ops
            my $stall  = $ins =~ /LDS|TLD|STS|BAR/ ? 0 : 1;

            my $ctrl   = "$waitDB:-:-:$yield:$stall";

            # output our FFMA and also any inserted ops
            $out .= sprintf "%s      FFMA c%s, j%dA%s, j%dB%s, c%s;%s\n%s", $ctrl, $cOrder[$c], $odd, $x, $odd, $y, $cOrder[$c], $comment, $ins;
        }
    }
    return $out;

</CODE>

// Main loop is done, time to write C to global memory.
<SCHEDULE_BLOCK>

--:-:-:-:1      MOV ldc, c[0x0][0x158];
--:-:-:-:1      MOV m,   c[0x0][0x144];
--:-:-:-:1      MOV n,   c[0x0][0x148];

// Remove the high bits if present from the last loop's xor
--:-:-:-:1      LOP.AND readAs, readAs, 0xfff;
--:-:-:-:1      LOP.AND readBs, readBs, 0xfff;

// Remap readAs and readBs onto writeCs so we can shuffle the output for coalesced global writes.
// readAs stays constant, readBs colapses down from stride 4 to 1
// writeCs = (readBs / 4) * 128 + readAs;
--:-:-:-:1      ISCADD  writeCs, readBs, readAs, 5;

// Read out the C values from shared in a simple tid mapped pattern but
// offset by the position of this warp's data in shared.

// cx = tid31 | (tid128 >> 2);
--:-:-:-:1      SHR.U32  cx, tid128, 2;
--:-:-:-:1      LOP.OR   cx, tid31,  cx;

// readCs = ((tid96 << 4) | cx) << 2;
--:-:-:-:1      SHL      readCs, tid96,  4;
--:-:-:-:1      LOP.OR   readCs, readCs, cx;
--:-:-:-:1      SHL      readCs, readCs, 2;

// cx += bx*128;
--:-:-:-:1      ISCADD  cx, bx, cx, 7;

// cy = by*128 + (tid96 >> 1)
--:-:-:-:1      SHR.U32 cy00, tid96, 1;
--:-:-:-:1      ISCADD  cy00, by, cy00, 7;

// C += (cy*ldc + cx) * 4;
--:-:-:-:1      XMAD.LO ci, cy00, ldc, cx, xmad2;
--:-:-:-:1      ISCADD  Cy00, ci, c[0x0][0x140], 2;

// When writing in assembly, being able to 'printf' is sometimes easier than stepping through the debugger.
// Here's how it's done.  Drop something like this in your code (math assumes 256 threads and 1 block).
// Then modify the c code to accept this many params per thread to printf (see assemblySgemm function).
//--:-:-:-:1      ISCADD D, tid, c[0x0][0x15c], 0x2;
//--:-:-:-:1      STG.CS [D + 4x<0 * 256>], readAs;
//--:-:-:-:1      STG.CS [D + 4x<1 * 256>], readBs;
//--:-:-:-:1      STG.CS [D + 4x<2 * 256>], writeCs;
//--:-:-:-:1      STG.CS [D + 4x<3 * 256>], readCs;
//--:-:-:-:1      STG.CS [D + 4x<4 * 256>], cx;
//--:-:-:-:1      STG.CS [D + 4x<5 * 256>], cy00;
//--:-:-:-:1      STG.CS [D + 4x<6 * 256>], ci;
//--:-:-:-:1      STG.CS [D + 4x<7 * 256>], cx67y67;

// Setup our matrix bounds checking vars and preds
--:-:-:-:1      ISETP.LT.AND P5, PT, cx, m, PT;
--:-:-:-:1      IADD cx, cx, 64;
--:-:-:-:1      ISETP.LT.AND P6, PT, cx, m, PT;

--:-:-:-:1      IADD cy00, cy00, -1;
--:-:-:-:1      IADD cy04, cy00,  4;
--:-:-:-:1      IADD cy08, cy00,  8;
--:-:-:-:1      IADD cy12, cy00,  12;

// Setup our C output addresses and increments.
--:-:-:-:1      SHL  ldc1,  ldc, 2;
--:-:-:-:1      SHL  ldc4,  ldc, 4;
--:-:-:-:1      SHL  ldc8,  ldc, 5;
--:-:-:-:1      ISCADD ldc60, ldc, -ldc4, 8;

// We pre-increment the output addresses so they can be dual issued with memory ops
// So start with a -1 instead of 0 value.
--:-:-:-:1      IADD Cy00, Cy00, -ldc1;
--:-:-:-:1      IADD Cy04, Cy00, ldc4;
--:-:-:-:1      IADD Cy08, Cy00, ldc8;
--:-:-:-:0      IADD Cy12, Cy04, ldc8; // Dual Issue (last instruction after reordering)

// Load the first set of the STORE_C subroutine params in the scheduled block.
--:-:-:-:1      MOV cs0, cx00y00;
--:-:-:-:1      MOV cs1, cx01y00;
--:-:-:-:1      MOV cs2, cx02y00;
--:-:-:-:1      MOV cs3, cx03y00;
--:-:-:-:1      MOV cs4, cx64y00;
--:-:-:-:1      MOV cs5, cx65y00;
--:-:-:-:1      MOV cs6, cx66y00;
--:-:-:-:1      MOV cs7, cx67y00;

</SCHEDULE_BLOCK>

// There's nothing yet in place to handle dependecies with subroutines.
// So don't schedule this block.
<CODE>

    my $out;
    foreach my $y (0..3, 64..67)
    {
        my ($wait, $comment) = $y == 64 ? ('--', '') : ('02',' // Wait Dep 2');

        # Jump ahead 60 units (to get to the values at y=64)
        $out .=
            "--:-:-:-:1      IADD cy00, cy00, 60;\n" .
            "--:-:-:-:1      IADD cy04, cy04, 60;\n" .
            "--:-:-:-:1      IADD cy08, cy08, 60;\n" .
            "--:-:-:-:1      IADD cy12, cy12, 60;\n\n" .

            "02:-:-:-:1      IADD Cy00, Cy00, ldc60; // Wait Dep 2\n" .
            "--:-:-:-:1      IADD Cy04, Cy04, ldc60;\n" .
            "--:-:-:-:1      IADD Cy08, Cy08, ldc60;\n" .
            "--:-:-:-:1      IADD Cy12, Cy12, ldc60;\n\n"  if $y == 64;

        # We need to move the C values to the param registers of the STORE_C subroutine.
        # Note that these could be FMUL's instead of MOV's if we wanted to multiply the output by alpha.
        $out .= sprintf(
            "%s:-:-:-:1      MOV cs0, cx00y%02d;%s\n" .
            "--:-:-:-:1      MOV cs1, cx01y%02d;\n" .
            "--:-:-:-:1      MOV cs2, cx02y%02d;\n" .
            "--:-:-:-:1      MOV cs3, cx03y%02d;\n" .
            "--:-:-:-:1      MOV cs4, cx64y%02d;\n" .
            "--:-:-:-:1      MOV cs5, cx65y%02d;\n" .
            "--:-:-:-:1      MOV cs6, cx66y%02d;\n" .
            "--:-:-:-:0      MOV cs7, cx67y%02d; // Dual Issue\n",
            $wait, $y, $comment, ($y) x 7) if $y;

        # Call the subroutine.
        $out .= "--:-:-:-:5      CAL STORE_C;\n\n";
    }
    return $out;

</CODE>

// And we'd done.  The remainder is the STORE_C subroutine that's defined at the end of the kernel.
--:-:-:-:5      EXIT;

// This routine does warp synchronous shuffling of our output data so as to be able
// to have coalesced writes to global memory.  This is actually faster because the shared
// memory latencies can be hidden by other warps and we're only adding a few extra clocks
// to this thread.  Global memory here is the bottleneck and being able to half the needed
// bandwidth at the expense of a few clocks is a modest win.  This also keeps power lower
// and our chip running faster.

// Note, the SHFL instruction doesn't help us here because we're swaping different registers
// from different threads.
STORE_C:

<SCHEDULE_BLOCK>

// Each warp writes to its own region of memory so we don't need to bar.sync the access.
// There are some bank conflicts here on the STS.128s but no way to avoid them, and the hit just means a few extra clocks.
// Note here that the scheduler is able to handle the dependencies between vector and non-vector instructions.
// It knows from the instruction type and the register map that cs0 here includes cs1, cs2 and cs3 as well.
--:-:-:-:1      STS.128 [writeCs+4x<00>], cs0;
--:-:-:-:1      STS.128 [writeCs+4x<64>], cs4;

// Loads naturally occur after the store to shared completes, no sync required.
--:-:-:-:1      LDS cs0, [readCs + 4x<0*128 + 00>];
--:-:-:-:1      LDS cs1, [readCs + 4x<0*128 + 64>];
--:-:-:-:1      LDS cs2, [readCs + 4x<1*128 + 00>];
--:-:-:-:1      LDS cs3, [readCs + 4x<1*128 + 64>];
--:-:-:-:1      LDS cs4, [readCs + 4x<2*128 + 00>];
--:-:-:-:1      LDS cs5, [readCs + 4x<2*128 + 64>];
--:-:-:-:1      LDS cs6, [readCs + 4x<3*128 + 00>];
--:-:1:-:1      LDS cs7, [readCs + 4x<3*128 + 64>]; // Set Dep 1

--:-:-:-:1      IADD cy00, cy00, 1;
--:-:-:-:1      IADD cy04, cy04, 1;
--:-:-:-:1      IADD cy08, cy08, 1;
--:-:-:-:1      IADD cy12, cy12, 1;

--:-:-:-:1      IADD Cy00, Cy00, ldc1;
--:-:-:-:1      IADD Cy04, Cy04, ldc1;
--:-:-:-:1      IADD Cy08, Cy08, ldc1;
--:-:-:-:1      IADD Cy12, Cy12, ldc1;

--:-:-:-:1      ISETP.LT.AND P0, PT, cy00, n, P5;
--:-:-:-:1      ISETP.LT.AND P1, PT, cy00, n, P6;
--:-:-:-:1      ISETP.LT.AND P2, PT, cy04, n, P5;
--:-:-:-:1      ISETP.LT.AND P3, PT, cy04, n, P6;

01:-:-:-:1  @P0 STG.CG [Cy00 + 4x<00>], cs0; // Wait Dep 1
--:-:-:-:1  @P1 STG.CG [Cy00 + 4x<64>], cs1;
--:-:-:-:1  @P2 STG.CG [Cy04 + 4x<00>], cs2;
--:-:-:-:1  @P3 STG.CG [Cy04 + 4x<64>], cs3;

--:-:-:-:1      ISETP.LT.AND P0, PT, cy08, n, P5;
--:-:-:-:1      ISETP.LT.AND P1, PT, cy08, n, P6;
--:-:-:-:1      ISETP.LT.AND P2, PT, cy12, n, P5;
--:-:-:-:1      ISETP.LT.AND P3, PT, cy12, n, P6;

--:-:-:-:1  @P0 STG.CG [Cy08 + 4x<00>], cs4;
--:-:-:-:1  @P1 STG.CG [Cy08 + 4x<64>], cs5;
--:-:-:-:1  @P2 STG.CG [Cy12 + 4x<00>], cs6;
--:2:-:-:1  @P3 STG.CG [Cy12 + 4x<64>], cs7; // Set Dep 2

</SCHEDULE_BLOCK>

--:-:-:-:5      RET;

